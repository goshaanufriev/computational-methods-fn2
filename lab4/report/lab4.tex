% !TeX spellcheck = ru_RU-Russian
\documentclass[12pt, a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[russian]{babel}

\usepackage[oglav, boldsect, eqwhole, figwhole, %
remarks, hyperref, hyperprint]{fn2kursstyle}

\makeatletter
\renewcommand{\@oddhead}{\vbox{Ануфриев Г.А., Брегадзе З.Г. \hfill ФН2-52Б \hrule}}
\makeatother

\begin{document}
	\section*{Ответы на контрольные вопросы}
	\begin{enumerate}
		\item\textbf{Почему нельзя находить собственные числа матрицы $A$, прямо решая уравнение $\det(A-\lambda E)=0$, а собственные векторы --- <<по определению>>, решая систему $(A-\lambda_jE)e_j=0$?}
		
		Так как $A$ --- матрица размера $n\times n$, то $\det(A-\lambda E)$ --- многочлен степени $n$ относительно $\lambda$, и для $n\geq5$ уже не существует аналитической формулы решения, поэтому в общем случае решить это уравнение не получится.
		
		Вычисление определителя неустойчиво, так как погрешности в элементах матрицы могут привести к неверному результату.
		
		Нахождение собственных векторов невозможно по определению, так как из найденного $\lambda_j$ можно найти приближенные решения уравнения $\det(A-\lambda E)=0$, которые могут сильно отклониться от истинного решения из-за накопившихся ошибок.
		
		\item\textbf{Докажите, что ортогональное преобразование подобия сохраняет симметрию матрицы}.
		
		Пусть $A$ --- симметричная матрица, $P$ --- ортогональная матрица, то есть
		\begin{align*}
			A^T&=A & P^T P&=E.
		\end{align*}
		
		Если $R$ --- матрица, полученная с помощью ортогонального преобразования подобия, то её можно записать в виде
		
		$$
		R=P^T AP.
		$$
		
		Запишем $R^T$:
		\[
		R^T=(P^T AP)^T=(AP)^T(P^T)^T=P^T A^T P = P^T A P = R.
		\]
		
		\item\textbf{Как преобразование подобия меняет собственные векторы матрицы?}
		
		Преобразование подобия является линейным преобразованием, то есть оно масштабирует и поворачивает собственные векторы.
		
		Пусть $A$ и $B$ --- подобные матрицы, то есть существует такая матрица $P$: $B=P^{-1}AP$.
		
		Пусть $x$ --- собственный вектор матрицы $A$, то есть $Ax=\lambda x$; $y=P^{-1}x$. Тогда
		
		\[
		By=(P^{-1}AP)y=P^{-1}APP^{-1}x=P^{-1}Ax=P^{-1}\lambda x=\lambda P^{-1}x=\lambda y.
		\]
		
		Получили, что $y=P^{-1}x$ --- собственный вектор матрицы $B$ при том же собственном значении $\lambda$.
		
		\item\textbf{Почему на практике матрицу $A$ подобными преобразованиями вращения приводят только к форме Хессенберга, но не к треугольному виду?}
		
		Приведение матрицы $A$ методом вращений к матрице Хессенберга позволяет получить более простую матрицу $B$, собственные значения которой совпадают с собственными значениями исходной матрицы. Поэтому будет достаточно найти собственные числа матрицы $B$ с помощью $QR$-разложения.
		
		Приведение матрицы $A$ к верхнетреугольному виду позволило бы сразу найти собственные значения матрицы $A$, но в общем случае это сделать ортогональными преобразованиями невозможно.
		
		\item\textbf{Оцените количество арифметических операций, необходимое для приведения произвольной квадратной матрицы $A$ к форме Хессенберга.}
		
		\item\textbf{Сойдется ли алгоритм обратных итераций, если в качестве начального приближения взять собственный вектор, соответствующий другому собственному значению? Что будет в этой ситуации в методе обратной итерации, использующем отношение Рэлея?}
		
%		Рассмотрим метод обратных итераций при фиксированном $\lambda_i^*$ (достаточно хорошем приближении $\lambda_i$):
%		
%		\begin{gather*}
%			(A-\lambda_i^*E)y^{(k+1)}=x^{(k)}, \\
%			x^{(k+1)}=\frac{y^{(k+1)}}{\|y^{(k+1)}\|}.
%		\end{gather*}
%		
%		Пусть $v_i$ --- собственный вектор, соответствующий собственному числу $\lambda_i$. Тогда используя метод обратных итераций, получим
%		
%		\[
%		(A-\lambda^*_i)y_1=v_i
%		\]
%		
%		Так как $v_i$ --- собственный вектор, то 
%		$$(A-\lambda^*_i E)v_i=(\lambda_i-\lambda_i^*)v_i$$.
%		
%		После преобразований получим:
%		
%		\[
%		y_1=\frac{1}{\lambda_i-\lambda_i^*}\,v_i
%		\]
%		
%		Произведём нормировку:
%		
%		\[
%		\|x_1\|=\frac{y_1}{\|y_1\|}=v_i
%		\]
%		
%		Таким образом, полученный вектор останется тем же для любой итерации, и метод не сойдется к другому собственному вектору.
%		
%		При рассмотрении метода Рэлея на $k$-м шаге число $\lambda^{(k)}$ представимо в виде
%		
%		\[
%		\lambda^{(k)}=\frac{(Ax^{(k)},x^{(k)})}{(x^{(k)},x^{(k)})}
%		\]
%		
%		Тогда, если в качестве начального приближения взять $x^{(0)}=v_i$, получим
%		
%		\[
%		\lambda^{(0)} = \frac{(Av_i,v_i)}{(v_i,v_i)}=\frac{v_i^T A v_i}{v_i^T v_i} = \frac{v_i^T \lambda_i v_i}{v_i^T v_i}=\lambda_i
%		\]
%		
%		Так как сразу получено точное собственное значение, то процесс остановится и метод сойдется к исходному собственному вектору.
		
		\item\textbf{Сформулируйте и обоснуйте критерий останова для $QR$-алгоритма отыскания собственных значений матрицы.}
		
		\item\textbf{Предложите возможные варианты условий перехода к алгоритму со сдвигами. Предложите алгоритм выбора величины сдвига.}
		
		\item\textbf{Для чего нужно на каждой итерации нормировать приближение к собственному вектору?}
		
		
		
		\item\textbf{Приведите примеры использования собственных чисел и собственных векторов в численных методах.}
		
		\begin{itemize}
			\item Оценка числа обусловленности матрицы $A$: $\text{cond}A\geq\dfrac{|\lambda_{max}|}{|\lambda_{min}|}$
			
			\item Нахождение оптимального параметра $\tau$ при решении СЛАУ с матрицей системы $A=A^T>0$ методом простых итераций: $\tau_{opt}=\dfrac{2}{\lambda_{max}+\lambda_{min}}$
			
%			\item Вычисление нормы матрицы $\|A\|_2=\sup\limits_{\|x\|_2=1}\|Ax\|_2=\sqrt{\max\limits_{i=\overline{1,n}}\lambda_i}$, где $\lambda_i$ --- собственные числа матрицы $A^T A$.
		\end{itemize}
		
	\end{enumerate}
\end{document}